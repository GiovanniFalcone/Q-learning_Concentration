# ENV
NUMBER_EPISODES = 100000    # agent training
EPISODES_WITH_HUMAN = 1     # games with human

# Robot type
TOM = 0
NO_TOM = 1
DECEPTION = 2

# Constraint for first flip suggestion
CLICKS_UNTIL_MATCH_THRESHOLD_WITH_PAIR = 4
CLICKS_UNTIL_MATCH_THRESHOLD_WITHOUT_PAIR = 9

# Action
SUGGEST_NONE = 0
SUGGEST_ROW = 1
SUGGEST_COL = 2
SUGGEST_CARD = 3

# Reward for suggestions
REWARD_SUGGEST_NONE = 10
REWARD_SUGGEST_ROW =  0.1
REWARD_SUGGEST_COL = 0.2
REWARD_SUGGEST_CARD = 0.025

# Constants for game state
BEGIN_STATE = 3 
MIDDLE_STATE = 2 
END_STATE = 1

# Q-table states
INIT_STATE = 0

NO_HELP_BEG_F_CORRECT = 1
NO_HELP_BEG_F_WRONG = 2
NO_HELP_MID_F_CORRECT = 3
NO_HELP_MID_F_WRONG = 4
NO_HELP_END_F_CORRECT = 5
NO_HELP_END_F_WRONG = 6

SUGG_ROW_BEG_F_CORRECT = 7
SUGG_ROW_BEG_F_WRONG = 8
SUGG_ROW_MID_F_CORRECT = 9
SUGG_ROW_MID_F_WRONG  = 10
SUGG_ROW_END_F_CORRECT  = 11
SUGG_ROW_END_F_WRONG = 12

SUGG_COL_BEG_F_CORRECT = 13
SUGG_COL_BEG_F_WRONG = 14
SUGG_COL_MID_F_CORRECT = 15
SUGG_COL_MID_F_WRONG  = 16
SUGG_COL_END_F_CORRECT  = 17
SUGG_COL_END_F_WRONG = 18

SUGG_CARD_BEG_F_CORRECT = 19
SUGG_CARD_BEG_F_WRONG = 20          # always 0: if the agent suggest the second card then it's 100% match
SUGG_CARD_MID_F_CORRECT = 21
SUGG_CARD_MID_F_WRONG = 22          # always 0
SUGG_CARD_END_F_CORRECT = 23
SUGG_CARD_END_F_WRONG = 24          # always 0

NO_HELP_BEG_S_CORRECT = 25
NO_HELP_BEG_S_WRONG = 26
NO_HELP_MID_S_CORRECT = 27
NO_HELP_MID_S_WRONG = 28
NO_HELP_END_S_CORRECT = 29
NO_HELP_END_S_WRONG = 30

SUGG_ROW_BEG_S_CORRECT = 31
SUGG_ROW_BEG_S_WRONG = 32
SUGG_ROW_MID_S_CORRECT = 33
SUGG_ROW_MID_S_WRONG  = 34
SUGG_ROW_END_S_CORRECT  = 35
SUGG_ROW_END_S_WRONG = 36

SUGG_COL_BEG_S_CORRECT = 37
SUGG_COL_BEG_S_WRONG = 38
SUGG_COL_MID_S_CORRECT = 39
SUGG_COL_MID_S_WRONG  = 40
SUGG_COL_END_S_CORRECT  = 41
SUGG_COL_END_S_WRONG = 42

SUGG_CARD_BEG_S_CORRECT = 43
SUGG_CARD_BEG_S_WRONG = 44        
SUGG_CARD_MID_S_CORRECT = 45
SUGG_CARD_MID_S_WRONG = 46        
SUGG_CARD_END_S_CORRECT = 47
SUGG_CARD_END_S_WRONG = 48        